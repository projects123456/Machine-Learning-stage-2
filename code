# Import basic libraries needed for this assignment
%config Completer.use_jedi = False  # enable code auto-completion
import numpy as np #import numpy to work with arrays
import pandas as pd #import pandas to manipulate the dataset
from matplotlib import pyplot as plt #import the module matplotlib.pyplot to do visulization
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import PolynomialFeatures    # function to generate polynomial and interaction features
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor


# import the data of happiness:
df = pd.read_csv('world-happiness-report-2021.csv')

# Selection of columns we want to use:
# The feautre is the GDP per capita:
X = df['Logged GDP per capita'].to_numpy().reshape(-1, 1)
y = df['Ladder score'].to_numpy()

# Split to training set (50% with random_state=42)
X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.4, random_state=42)
# Split to test and validation set from the remaining data.
X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=42)

degrees = [2,3,4,5,6,7,8,9,10]    

# variable to store the training errors:
training_errors = []
# variable to store the validiation error:
validiation_errors = []

# generate a new figure window
plt.figure(figsize=(20, 10))  


# Modeling with different polynomial degrees by using for-loop
for i in range(len(degrees)):    
    plt.subplot(3, 3, i + 1)    # choose the subplot
    
    # Initialising the LinearRegression
    lin_regr = LinearRegression(fit_intercept=False)
    
    # Generating the polynomial features for degree
    poly = PolynomialFeatures(degree=degrees[i])    
    
       # Training:
    # The features fit and transformed:
    X_training_poly = poly.fit_transform(X_train) 
    # Fitting the model to the new features and the original labels
    lin_regr.fit(X_training_poly, y_train)    
    # Predicting new y-values from the model:
    y_predicted_training = lin_regr.predict(X_training_poly)
    
    # MSE for training:
    training_error = mean_squared_error(y_train, y_predicted_training)
    training_errors.append(training_error)
       
        # Validiation:
    # Transforming the validiation featrues:
    X_validiation_poly = poly.transform(X_val)
    # Predicting the validiation y-values:
    y_predicted_validiation = lin_regr.predict(X_validiation_poly)
    
    # MSE for validiation
    validiation_error = mean_squared_error(y_val, y_predicted_validiation)
    validiation_errors.append(validiation_error)
    
    # Test-set:
    X_test_poly = poly.transform(X_test)
    y_predicted_test = lin_regr.predict(X_validiation_poly)
    test_error = mean_squared_error(y_test, y_predicted_test)
    
    X_fit = np.linspace(6, 12, 10) 
    plt.tight_layout()
    # Scatter plot for training data (datapoints visualized):
    plt.scatter(X_train, y_train, color="b", s=10, label="Train Datapoints")   
    # Scatter plot for validiation data:
    plt.scatter(X_val, y_val, color="r", s=10, label="Validation Datapoints")   
    # Plot of model:
    plt.plot(X_fit, lin_regr.predict(poly.transform(X_fit.reshape(-1, 1))), label="Model")
    plt.legend(loc="best")
    # Labels for x-axis and y-axis and the title for the figure
    plt.xlabel('Loc GDP per capita')    
    plt.ylabel('Ladder score')
    plt.title(f'Polynomial degree = {degrees[i]}\nTraining error = {training_error:.5}\nValidation error = {validiation_error:.5}\nTest error ={test_error}') 

    i += 1
plt.show()    

plt.figure(figsize=(20, 10)) 


# Decision tree regression analysis:
depths = [2,3,4,5,6,7]    
for i in range(len(depths)):    
    plt.subplot(2, 3, i + 1)    # choose the subplot
    
    # Initialising the Decision Tree Regression
    regressor = DecisionTreeRegressor(max_depth=depths[i])
    
    regressor.fit(X_train,y_train)
    # Predicting the y-training:
    y_predicted_train_tree = regressor.predict(X_train)
    # Training error:
    training_error_tree = mean_squared_error(y_train.reshape(-1,1), y_predicted_training.reshape(-1,1))
    
    # Predicting the y-training:
    y_predicted_val_tree = regressor.predict(X_val)
    # Validiation error: 
    validiation_error_tree = mean_squared_error(y_val.reshape(-1,1), y_predicted_val_tree.reshape(-1,1))
    
    # Predicting the y-test:
    y_predicted_test_tree = regressor.predict(X_test)
    # Test error:
    test_error_tree = mean_squared_error(y_test.reshape(-1,1), y_predicted_test_tree.reshape(-1,1))       
    
    X_fit = np.linspace(6, 12, 10) 
    plt.tight_layout()
    # Scatter plot for training data (datapoints visualized):
    plt.scatter(X_train, y_train, color="b", s=10, label="Train Datapoints")   
    # Scatter plot for validiation data:
    plt.scatter(X_val, y_val, color="r", s=10, label="Validation Datapoints")   
    # Plot of model:
    plt.plot(X_fit, regressor.predict(X_fit.reshape(-1, 1)), color="cornflowerblue", linewidth=2)
    plt.legend(loc="best")
    # Labels for x-axis and y-axis and the title for the figure
    plt.xlabel('Loc GDP per capita')    
    plt.ylabel('Ladder score')
    plt.title(f'Max depth = {depths[i]}\nTraining error = {training_error_tree:.5}\nValidation error = {validiation_error_tree:.5}\nTest error ={test_error_tree}')
    i += 1
    
plt.show()    
